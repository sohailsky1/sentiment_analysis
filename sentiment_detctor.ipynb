{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sohail/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 16:06:31.826497: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730891191.841030   11468 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730891191.845452   11468 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-06 16:06:31.859815: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        \n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, False)\n",
    "            tf.config.experimental.set_virtual_device_configuration(\n",
    "                gpu,\n",
    "                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])  # 4GB memory\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('/home/sohail/emotion/training.csv')\n",
    "validation_data = pd.read_csv('/home/sohail/emotion/validation.csv')\n",
    "testing_data = pd.read_csv('/home/sohail/emotion/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses a single text input using simple tokenization\n",
    "    \"\"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "   \n",
    "    text = text.lower()\n",
    "    \n",
    "    \n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    \n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "   \n",
    "    tokens = text.split()\n",
    "    \n",
    "    return text, tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(df, text_column='text', label_column='label', dataset_name=''):\n",
    "    \"\"\"\n",
    "    Process a single dataset and return statistics\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Processing {dataset_name} ===\")\n",
    "    print(f\"Initial shape: {df.shape}\")\n",
    "    \n",
    "   \n",
    "    initial_size = len(df)\n",
    "    df = df.dropna(subset=[text_column, label_column])\n",
    "    print(f\"Rows with missing values removed: {initial_size - len(df)}\")\n",
    "    \n",
    "    \n",
    "    df['cleaned_text'], df['tokens'] = zip(*df[text_column].apply(preprocess_text))\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_token_length = df['tokens'].apply(len).mean()\n",
    "    print(f\"Average tokens per text: {avg_token_length:.2f}\")\n",
    "    \n",
    "    # Show label distribution\n",
    "    print(\"\\nLabel distribution:\")\n",
    "    label_dist = df[label_column].value_counts().sort_index()\n",
    "    total = len(df)\n",
    "    \n",
    "    # Updated emotion mapping\n",
    "    emotion_map = {\n",
    "        0: 'sadness',\n",
    "        1: 'joy',\n",
    "        2: 'love',\n",
    "        3: 'anger',\n",
    "        4: 'fear',\n",
    "        5: 'surprise'  # Added new emotion\n",
    "    }\n",
    "    \n",
    "    for label, count in label_dist.items():\n",
    "        percentage = (count/total) * 100\n",
    "        emotion = emotion_map.get(label, f'unknown_{label}')  # Safely handle any unexpected labels\n",
    "        print(f\"{emotion} ({label}): {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Print unique labels for verification\n",
    "    print(\"\\nUnique labels in dataset:\", sorted(df[label_column].unique()))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Training Data ===\n",
      "Initial shape: (16000, 2)\n",
      "Rows with missing values removed: 0\n",
      "Average tokens per text: 19.17\n",
      "\n",
      "Label distribution:\n",
      "sadness (0): 4666 (29.2%)\n",
      "joy (1): 5362 (33.5%)\n",
      "love (2): 1304 (8.2%)\n",
      "anger (3): 2159 (13.5%)\n",
      "fear (4): 1937 (12.1%)\n",
      "surprise (5): 572 (3.6%)\n",
      "\n",
      "Unique labels in dataset: [0, 1, 2, 3, 4, 5]\n",
      "\n",
      "=== Processing Validation Data ===\n",
      "Initial shape: (2000, 2)\n",
      "Rows with missing values removed: 0\n",
      "Average tokens per text: 18.87\n",
      "\n",
      "Label distribution:\n",
      "sadness (0): 550 (27.5%)\n",
      "joy (1): 704 (35.2%)\n",
      "love (2): 178 (8.9%)\n",
      "anger (3): 275 (13.8%)\n",
      "fear (4): 212 (10.6%)\n",
      "surprise (5): 81 (4.0%)\n",
      "\n",
      "Unique labels in dataset: [0, 1, 2, 3, 4, 5]\n",
      "\n",
      "=== Processing Testing Data ===\n",
      "Initial shape: (2000, 2)\n",
      "Rows with missing values removed: 0\n",
      "Average tokens per text: 19.15\n",
      "\n",
      "Label distribution:\n",
      "sadness (0): 581 (29.0%)\n",
      "joy (1): 695 (34.8%)\n",
      "love (2): 159 (8.0%)\n",
      "anger (3): 275 (13.8%)\n",
      "fear (4): 224 (11.2%)\n",
      "surprise (5): 66 (3.3%)\n",
      "\n",
      "Unique labels in dataset: [0, 1, 2, 3, 4, 5]\n",
      "\n",
      "=== Sample of processed training data ===\n",
      "                                        cleaned_text  \\\n",
      "0                            i didnt feel humiliated   \n",
      "1  i can go from feeling so hopeless to so damned...   \n",
      "\n",
      "                                              tokens  \n",
      "0                       [i, didnt, feel, humiliated]  \n",
      "1  [i, can, go, from, feeling, so, hopeless, to, ...  \n"
     ]
    }
   ],
   "source": [
    "processed_training = process_dataset(training_data, dataset_name='Training Data')\n",
    "processed_validation = process_dataset(validation_data, dataset_name='Validation Data')\n",
    "processed_testing = process_dataset(testing_data, dataset_name='Testing Data')\n",
    "\n",
    "# Show sample of processed data\n",
    "print(\"\\n=== Sample of processed training data ===\")\n",
    "print(processed_training[['cleaned_text', 'tokens']].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureEngineering:\n",
    "    def __init__(self, max_features=5000, max_length=1000):\n",
    "        self.max_features = max_features\n",
    "        self.max_length = max_length\n",
    "        self.tfidf = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "            stop_words='english'\n",
    "        )\n",
    "        self.tokenizer = Tokenizer(num_words=max_features)\n",
    "        \n",
    "    def create_tfidf_features(self, train_texts, val_texts=None, test_texts=None):\n",
    "        \"\"\"\n",
    "        Create TF-IDF features for text data\n",
    "        \"\"\"\n",
    "        print(\"Creating TF-IDF features...\")\n",
    "        # Fit and transform training data\n",
    "        X_train_tfidf = self.tfidf.fit_transform(train_texts)\n",
    "        \n",
    "        # Transform validation and test if provided\n",
    "        X_val_tfidf = self.tfidf.transform(val_texts) if val_texts is not None else None\n",
    "        X_test_tfidf = self.tfidf.transform(test_texts) if test_texts is not None else None\n",
    "        \n",
    "        # Get feature names for analysis\n",
    "        feature_names = self.tfidf.get_feature_names_out()\n",
    "        print(f\"Number of TF-IDF features: {len(feature_names)}\")\n",
    "        \n",
    "        return {\n",
    "            'train': X_train_tfidf,\n",
    "            'val': X_val_tfidf,\n",
    "            'test': X_test_tfidf,\n",
    "            'feature_names': feature_names\n",
    "        }\n",
    "    \n",
    "    def create_sequence_features(self, train_texts, val_texts=None, test_texts=None):\n",
    "        \"\"\"\n",
    "        Create sequence features for deep learning models\n",
    "        \"\"\"\n",
    "        print(\"Creating sequence features...\")\n",
    "        # Fit tokenizer on training data\n",
    "        self.tokenizer.fit_on_texts(train_texts)\n",
    "        \n",
    "        # Convert texts to sequences\n",
    "        X_train_seq = self.tokenizer.texts_to_sequences(train_texts)\n",
    "        X_train_pad = pad_sequences(X_train_seq, maxlen=self.max_length, padding='post')\n",
    "        \n",
    "        # Process validation and test if provided\n",
    "        X_val_pad = None\n",
    "        if val_texts is not None:\n",
    "            X_val_seq = self.tokenizer.texts_to_sequences(val_texts)\n",
    "            X_val_pad = pad_sequences(X_val_seq, maxlen=self.max_length, padding='post')\n",
    "            \n",
    "        X_test_pad = None\n",
    "        if test_texts is not None:\n",
    "            X_test_seq = self.tokenizer.texts_to_sequences(test_texts)\n",
    "            X_test_pad = pad_sequences(X_test_seq, maxlen=self.max_length, padding='post')\n",
    "        \n",
    "        print(f\"Vocabulary size: {len(self.tokenizer.word_index) + 1}\")\n",
    "        print(f\"Sequence length: {self.max_length}\")\n",
    "        \n",
    "        return {\n",
    "            'train': X_train_pad,\n",
    "            'val': X_val_pad,\n",
    "            'test': X_test_pad,\n",
    "            'tokenizer': self.tokenizer\n",
    "        }\n",
    "    \n",
    "    def analyze_tfidf_features(self, tfidf_features, labels):\n",
    "        \"\"\"\n",
    "        Analyze the most important features for each emotion\n",
    "        \"\"\"\n",
    "        print(\"\\nAnalyzing important features for each emotion...\")\n",
    "        feature_names = tfidf_features['feature_names']\n",
    "        X_train_tfidf = tfidf_features['train']\n",
    "        \n",
    "        for emotion in sorted(set(labels)):\n",
    "            # Get indices for this emotion\n",
    "            emotion_indices = labels == emotion\n",
    "            \n",
    "            # Calculate average TF-IDF scores for this emotion\n",
    "            emotion_scores = X_train_tfidf[emotion_indices].mean(axis=0).A1\n",
    "            \n",
    "            # Get top features\n",
    "            top_indices = emotion_scores.argsort()[-10:][::-1]\n",
    "            top_features = [(feature_names[i], emotion_scores[i]) for i in top_indices]\n",
    "            \n",
    "            print(f\"\\nTop features for emotion {emotion}:\")\n",
    "            for feature, score in top_features:\n",
    "                print(f\"{feature}: {score:.4f}\")\n",
    "\n",
    "def create_features(processed_training, processed_validation, processed_testing):\n",
    "    \"\"\"\n",
    "    Create features for all datasets\n",
    "    \"\"\"\n",
    "    # Initialize feature engineering\n",
    "    feature_eng = TextFeatureEngineering(max_features=5000, max_length=100)\n",
    "    \n",
    "    print(\"Processing features for training set of shape:\", processed_training.shape)\n",
    "    \n",
    "    # Create TF-IDF features\n",
    "    tfidf_features = feature_eng.create_tfidf_features(\n",
    "        processed_training['cleaned_text'],\n",
    "        processed_validation['cleaned_text'],\n",
    "        processed_testing['cleaned_text']\n",
    "    )\n",
    "    \n",
    "    # Create sequence features for deep learning\n",
    "    sequence_features = feature_eng.create_sequence_features(\n",
    "        processed_training['cleaned_text'],\n",
    "        processed_validation['cleaned_text'],\n",
    "        processed_testing['cleaned_text']\n",
    "    )\n",
    "    \n",
    "    # Analyze features\n",
    "    feature_eng.analyze_tfidf_features(tfidf_features, processed_training['label'])\n",
    "    \n",
    "    return tfidf_features, sequence_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features for training set of shape: (16000, 4)\n",
      "Creating TF-IDF features...\n",
      "Number of TF-IDF features: 5000\n",
      "Creating sequence features...\n",
      "Vocabulary size: 15213\n",
      "Sequence length: 100\n",
      "\n",
      "Analyzing important features for each emotion...\n",
      "\n",
      "Top features for emotion 0:\n",
      "feel: 0.0608\n",
      "feeling: 0.0408\n",
      "like: 0.0276\n",
      "im: 0.0236\n",
      "feel like: 0.0208\n",
      "just: 0.0161\n",
      "really: 0.0119\n",
      "im feeling: 0.0118\n",
      "know: 0.0116\n",
      "ive: 0.0098\n",
      "\n",
      "Top features for emotion 1:\n",
      "feel: 0.0611\n",
      "feeling: 0.0363\n",
      "like: 0.0282\n",
      "im: 0.0242\n",
      "feel like: 0.0214\n",
      "im feeling: 0.0142\n",
      "just: 0.0136\n",
      "really: 0.0114\n",
      "time: 0.0109\n",
      "pretty: 0.0097\n",
      "\n",
      "Top features for emotion 2:\n",
      "feel: 0.0581\n",
      "feeling: 0.0354\n",
      "like: 0.0348\n",
      "feel like: 0.0265\n",
      "im: 0.0235\n",
      "loving: 0.0185\n",
      "love: 0.0170\n",
      "caring: 0.0168\n",
      "sweet: 0.0160\n",
      "sympathetic: 0.0159\n",
      "\n",
      "Top features for emotion 3:\n",
      "feel: 0.0569\n",
      "feeling: 0.0414\n",
      "like: 0.0273\n",
      "im: 0.0265\n",
      "feel like: 0.0221\n",
      "just: 0.0179\n",
      "im feeling: 0.0146\n",
      "angry: 0.0130\n",
      "irritable: 0.0126\n",
      "really: 0.0123\n",
      "\n",
      "Top features for emotion 4:\n",
      "feel: 0.0523\n",
      "feeling: 0.0465\n",
      "im: 0.0270\n",
      "like: 0.0190\n",
      "little: 0.0169\n",
      "im feeling: 0.0161\n",
      "just: 0.0153\n",
      "bit: 0.0145\n",
      "strange: 0.0134\n",
      "nervous: 0.0133\n",
      "\n",
      "Top features for emotion 5:\n",
      "feel: 0.0518\n",
      "feeling: 0.0435\n",
      "amazed: 0.0424\n",
      "impressed: 0.0394\n",
      "curious: 0.0369\n",
      "overwhelmed: 0.0346\n",
      "weird: 0.0339\n",
      "surprised: 0.0333\n",
      "shocked: 0.0327\n",
      "funny: 0.0309\n"
     ]
    }
   ],
   "source": [
    "tfidf_features, sequence_features = create_features(processed_training, processed_validation, processed_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 59  \n",
    "embedding_dim = 200  \n",
    "num_classes = 6\n",
    "max_features = 1000  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefficients = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefficients\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(tokenizer, embeddings_index, embedding_dim):\n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(embedding_matrix, max_length, num_classes):\n",
    "    model = Sequential()\n",
    "    # Embedding Layer\n",
    "    model.add(Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                        output_dim=embedding_matrix.shape[1],\n",
    "                        embeddings_initializer=Constant(embedding_matrix),\n",
    "                        input_length=max_length,\n",
    "                        trainable=False))  # Set trainable=True if you want to fine-tune\n",
    "    \n",
    "    # LSTM Layer 1\n",
    "    model.add(LSTM(512, return_sequences=True,))  # Increased units to 256\n",
    "    \n",
    "    # LSTM Layer 2\n",
    "    model.add(LSTM(256, return_sequences=False,))  # Increased units to 128\n",
    "    \n",
    "    # Dense Layer\n",
    "    model.add(Dense(128, activation='relu'))  # Increased units to 128\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    # Output Layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(processed_training['cleaned_text'])\n",
    "X_train_seq = tokenizer.texts_to_sequences(processed_training['cleaned_text'])\n",
    "X_val_seq = tokenizer.texts_to_sequences(processed_validation['cleaned_text'])\n",
    "X_test_seq = tokenizer.texts_to_sequences(processed_testing['cleaned_text'])\n",
    "glove_file = '/home/sohail/glove/glove.6B.200d.txt'\n",
    "\n",
    "# Pad the sequences\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=max_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "embedding_index = load_glove_embeddings(glove_file)  # Update path\n",
    "embedding_matrix = create_embedding_matrix(tokenizer, embedding_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# After fitting the tokenizer on your training data\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length in training set: 59\n",
      "Max sequence length in validation set: 56\n",
      "Max sequence length in test set: 54\n",
      "Overall max sequence length: 59\n"
     ]
    }
   ],
   "source": [
    "# Find the maximum sequence length in training, validation, and test sets\n",
    "max_seq_train = max(len(seq) for seq in X_train_seq)\n",
    "max_seq_val = max(len(seq) for seq in X_val_seq)\n",
    "max_seq_test = max(len(seq) for seq in X_test_seq)\n",
    "\n",
    "# Overall maximum sequence length across all datasets\n",
    "overall_max_seq = max(max_seq_train, max_seq_val, max_seq_test)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Max sequence length in training set: {max_seq_train}\")\n",
    "print(f\"Max sequence length in validation set: {max_seq_val}\")\n",
    "print(f\"Max sequence length in test set: {max_seq_test}\")\n",
    "print(f\"Overall max sequence length: {overall_max_seq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length in training set: 59\n",
      "Max sequence length in validation set: 56\n",
      "Max sequence length in test set: 54\n",
      "Overall max sequence length: 59\n"
     ]
    }
   ],
   "source": [
    "# Find the maximum sequence length in each set\n",
    "train_max_length = max(len(seq) for seq in X_train_seq)\n",
    "val_max_length = max(len(seq) for seq in X_val_seq)\n",
    "test_max_length = max(len(seq) for seq in X_test_seq)\n",
    "\n",
    "# Determine the overall maximum sequence length\n",
    "max_length = max(train_max_length, val_max_length, test_max_length)\n",
    "\n",
    "print(f\"Max sequence length in training set: {train_max_length}\")\n",
    "print(f\"Max sequence length in validation set: {val_max_length}\")\n",
    "print(f\"Max sequence length in test set: {test_max_length}\")\n",
    "print(f\"Overall max sequence length: {max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Extract labels from the datasets\n",
    "y_train = processed_training['label']\n",
    "y_val = processed_validation['label']\n",
    "y_test = processed_testing['label']\n",
    "\n",
    "# Convert labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the label encoder on the training data and transform all sets\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_val = label_encoder.transform(y_val)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert the numerical labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=6)  # Assuming 6 emotion classes\n",
    "y_val = to_categorical(y_val, num_classes=6)\n",
    "y_test = to_categorical(y_test, num_classes=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohail/projects/venv/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "I0000 00:00:1730891321.856978   11468 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4096 MB memory:  -> device: 0, name: Quadro T2000, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = build_lstm_model(embedding_matrix, max_length, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1730891405.332728   11947 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 63ms/step - accuracy: 0.3064 - loss: 1.6467 - val_accuracy: 0.3520 - val_loss: 1.5825\n",
      "Epoch 2/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 63ms/step - accuracy: 0.3188 - loss: 1.5835 - val_accuracy: 0.3710 - val_loss: 1.5694\n",
      "Epoch 3/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 63ms/step - accuracy: 0.3568 - loss: 1.5640 - val_accuracy: 0.4325 - val_loss: 1.4310\n",
      "Epoch 4/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 63ms/step - accuracy: 0.4801 - loss: 1.3954 - val_accuracy: 0.5150 - val_loss: 1.3092\n",
      "Epoch 5/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 63ms/step - accuracy: 0.5231 - loss: 1.2831 - val_accuracy: 0.5540 - val_loss: 1.1996\n",
      "Epoch 6/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 62ms/step - accuracy: 0.5462 - loss: 1.2088 - val_accuracy: 0.5765 - val_loss: 1.1185\n",
      "Epoch 7/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 62ms/step - accuracy: 0.5899 - loss: 1.1004 - val_accuracy: 0.6300 - val_loss: 1.0126\n",
      "Epoch 8/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 63ms/step - accuracy: 0.6306 - loss: 1.0103 - val_accuracy: 0.6225 - val_loss: 1.0059\n",
      "Epoch 9/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 63ms/step - accuracy: 0.6652 - loss: 0.9037 - val_accuracy: 0.6835 - val_loss: 0.8663\n",
      "Epoch 10/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 63ms/step - accuracy: 0.6911 - loss: 0.8439 - val_accuracy: 0.7190 - val_loss: 0.7823\n",
      "Epoch 11/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 62ms/step - accuracy: 0.7315 - loss: 0.7415 - val_accuracy: 0.7430 - val_loss: 0.7387\n",
      "Epoch 12/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 63ms/step - accuracy: 0.7558 - loss: 0.6774 - val_accuracy: 0.7615 - val_loss: 0.6506\n",
      "Epoch 13/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 63ms/step - accuracy: 0.7790 - loss: 0.6013 - val_accuracy: 0.7865 - val_loss: 0.6121\n",
      "Epoch 14/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 63ms/step - accuracy: 0.8019 - loss: 0.5549 - val_accuracy: 0.8120 - val_loss: 0.5280\n",
      "Epoch 15/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 63ms/step - accuracy: 0.8233 - loss: 0.4955 - val_accuracy: 0.8295 - val_loss: 0.5075\n",
      "Epoch 16/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 63ms/step - accuracy: 0.8387 - loss: 0.4664 - val_accuracy: 0.8380 - val_loss: 0.4968\n",
      "Epoch 17/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 63ms/step - accuracy: 0.8547 - loss: 0.4172 - val_accuracy: 0.8440 - val_loss: 0.4496\n",
      "Epoch 18/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 63ms/step - accuracy: 0.8568 - loss: 0.3991 - val_accuracy: 0.8415 - val_loss: 0.4662\n",
      "Epoch 19/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 62ms/step - accuracy: 0.8564 - loss: 0.3991 - val_accuracy: 0.8455 - val_loss: 0.4424\n",
      "Epoch 20/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 63ms/step - accuracy: 0.8668 - loss: 0.3611 - val_accuracy: 0.8505 - val_loss: 0.4298\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_pad, y_train, \n",
    "                    epochs=20, \n",
    "                    batch_size=64, \n",
    "                    validation_data=(X_val_pad, y_val),\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sentiment_anaylisis.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1730963724.176510    1795 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted sentiment: joy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# Load your trained model\n",
    "model = load_model('sentiment_anaylisis.keras')\n",
    "\n",
    "# Load the tokenizer from file\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "# Define the sentiment labels\n",
    "emotion_labels = {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}\n",
    "\n",
    "def predict_sentiment(user_input, tokenizer, max_length):\n",
    "    # Preprocess input (tokenizing and padding)\n",
    "    input_seq = tokenizer.texts_to_sequences([user_input])\n",
    "    input_pad = pad_sequences(input_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(input_pad)\n",
    "    \n",
    "    # Get the sentiment with the highest probability\n",
    "    predicted_label = np.argmax(prediction, axis=1)[0]\n",
    "    \n",
    "    # Map the predicted label to the sentiment\n",
    "    predicted_sentiment = emotion_labels[predicted_label]\n",
    "    \n",
    "    return predicted_sentiment\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396ms/step\n",
      "Predicted sentiment: anger\n"
     ]
    }
   ],
   "source": [
    "user_input = \"\"\n",
    "predicted_sentiment = predict_sentiment(user_input, tokenizer, max_length=63)\n",
    "print(f\"Predicted sentiment: {predicted_sentiment}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
